平时我们对运行中的系统，如果要检查他的JVM的整体运行情况，使用jstat，但是非常的不够直观

命令 jstat -gc PID
 S0C    S1C    S0U    S1U      EC       EU        OC         OU       MC     MU    CCSC   CCSU   YGC     YGCT    FGC    FGCT     GCT
5120.0 5120.0  0.0    0.0   33280.0  18417.1   55296.0     4692.0   21296.0 20964.5 2608.0 2502.5      3    0.015   1      0.021    0.035

S0C：这是From Survivor区的大小
S1C：这是To Survivor区的大小
S0U：这是From Survivor区当前使用的内存大小
S1U：这是To Survivor区当前使用的内存大小
EC：这是Eden区的大小
EU：这是Eden区当前使用的内存大小
OC：这是老年代的大小
OU：这是老年代当前使用的内存大小
MC：这是方法区（永久代、元数据区）的大小
MU：这是方法区（永久代、元数据区）的当前使用的内存大小
YGC：这是系统运行迄今为止的Young GC次数
YGCT：这是Young GC的耗时
FGC：这是系统运行迄今为止的Full GC次数
FGCT：这是Full GC的耗时
GCT：这是所有GC的总耗时


jmap -head PID
jmap -histo PID 查看对象的内存占用情况
jmap -dump:live,format=b,file=dump.hprof PID 生成内存快照


使用jhat 在浏览器中分析dump文件 localhost:7000 即可查看
jhat dump.hprof -port 7000



产生oom的地方：
1.方法区
    1.1 一个规模相对较大的系统，由于设置的参数过小，导致不够
    1.2 利用动态技术例如cglib，区生成一些类，没控制好导致类过多

2.虚拟机栈
    2.1 递归调用，导致不停的调用某个方法（死循环），无限制的递归肯定导致栈内存溢出

3.堆内存
    3.1 full gc后，老年代还是放不下。有限的内存空间存了过多的对象，且都是存活的
        一般场景：系统负载过高，比如并发量过大，或者是数据量过大，或者是出现了内存泄漏的情况


如何处理oom，一般公司会使用Zabbix、Open-Falcon之类的监控平台
另外，一盒成熟的监控系统，应该是具备机器负载的各种情况的，就和阿里云一样，cpu/io/磁盘等，另外一类就是业务指标了，比如刷单等
一台机器最重要的就是1.cpu 2.内存

通常我们都需对对产生oom的现场进行排查，重要的就是排查dump文件，
那么启动时候，我们需要加入启动参数：
-XX:+HeapDumpOnOutOfMemoryError   在OOM的时候自动dump内存快照出来
-XX:HeapDumpPath=/usr/local/app/oom  内存快照放到哪儿去

但是，对于栈内存，分析dump文件是没用的，dump文件主要是对分析一些对内存的占用，这个只需要区分析log日志即可

一般我们可以用mat工具来进行分析，先看占用内存最多的对象是谁，然后分析那个线程的调用栈，接着就可以看到是哪个方法引发的内存溢出了

看到oom，其他的别想，直接线上看log


比如，某个服务牵扯到rpc的调用，设置了timeout而且时间相对较长，一旦很多请求来了，然后被卡住，挤压多了，很容易oom
